{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "672f9c3f-bbf3-49d6-a3a2-2c4a412df92a",
   "metadata": {},
   "source": [
    "### Testing MatPlotLib under Jupyter\n",
    "\n",
    "- It is flakey when it comes to updating the figure with time lapse.\n",
    "   It tends to make new figures for each pause.\n",
    "- Also on some occassions figure is not displayer at all unless I restart Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcc1faa-8c25-4079-bb9f-48358c56e635",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = range(10)\n",
    "y1 = np.sin(x) \n",
    "y2 = np.cos(x)\n",
    "\n",
    "plt.ion() \n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(2)\n",
    "plt.xlabel(\"data count\") \n",
    "plt.ylabel(\"loss\")\n",
    "plt.grid() \n",
    "\n",
    "for i in range (10):\n",
    "    if (i % 2 == 0):\n",
    "        plt.plot (x, y1, '-o') \n",
    "    else:\n",
    "        plt.plot (x, y2, '-ro') \n",
    "    #plt.pause(2)\n",
    "\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d790a7-48ef-4bb1-9c63-d67aca0e7e01",
   "metadata": {},
   "source": [
    "### Simple Neural Network with \"torch\" - Test 1\n",
    "-  Creates a nn model and dumps the compiled version of it into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a320134-e122-49df-8398-8889cc25a8df",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint as pp\n",
    "\n",
    "class MyModule (torch.nn.Module):\n",
    "    \n",
    "        def __init__(self, N, M):\n",
    "            super(MyModule, self).__init__()\n",
    "            #Generate some random weights\n",
    "            self.weight = torch.nn.Parameter(torch.rand(N,M))\n",
    "\n",
    "        # Set  initial weigts \n",
    "        def forward(self, input):\n",
    "            if input.sum() > 0:\n",
    "                output = self.weight.mv(input)\n",
    "            else:\n",
    "                output = self.weight + input\n",
    "            return output\n",
    "\n",
    "# Compile model - static representation ??\n",
    "mymod = torch.jit.script(MyModule(3,4)) \n",
    "mymod.save(\"myscript.pt\") #save model and data for others to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37460b5e-e7b3-413a-853e-3c534dce7e9b",
   "metadata": {},
   "source": [
    "### Simple Neural Network with \"torch\" - Test 2\n",
    "-  Creates a nn model and dumps the compiled version of it into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f585a1ac-cb97-48ca-9855-e4f184b00458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint as pp\n",
    "\n",
    "# create a very simple nn\n",
    "#  just one input layer and one outputlayer\n",
    "#  \n",
    "\n",
    "inputL = nn.Linear (3, 1)  #  3 input nodes with one output\n",
    "pp.pprint (inputL)\n",
    "print (inputL.weight)\n",
    "print (inputL.bias)\n",
    "\n",
    "print (\"intialise bias to zero value\")\n",
    "print (\"apparently it helps nn\")\n",
    "nn.init.zeros_(inputL.bias)\n",
    "print(\"**  Bias Values\", inputL.bias)\n",
    "\n",
    "print(\"Now initialize weights too\")\n",
    "# sample frm -x to +x with uniform distribution where x = sqrt( 6 / (ni + no) )\n",
    "# inthis case ni = 3,  no = 1   .... this keeps std of weights and gradients in check\n",
    "# have to see how it does this.\n",
    "print(f\"*** with xavier_uniform_ method x = {np.sqrt(6/(1 + 3)):.3f}\")\n",
    "nn.init.xavier_uniform_(inputL.weight)\n",
    "print(\"*** xavier uniform weights: \" , inputL.weight)\n",
    "\n",
    "print(f\"*** with xavier_normalm_ method x = {np.sqrt(2/(1 + 3)):.3f}\")\n",
    "nn.init.xavier_uniform_(inputL.weight)\n",
    "print(\"*** xavier normal weights: \" , inputL.weight)\n",
    "\n",
    "\n",
    "# Let us create a calss for our nn\n",
    "class Net (nn.Module):\n",
    "    def __init__(self):     # constructor\n",
    "        super(Net, self).__init__()     # calling parent class\n",
    "        self.layer1 = nn.Linear (10, 5)\n",
    "        self.layer2 = nn.Linear (5, 2)\n",
    "\n",
    "    def forward (self, x):      # we propagate inputs through layers\n",
    "        x = F.relu (self.layer1 (x))   # what is this F object applying rlu from input layer\n",
    "        x = self.layer2 (x)            # pass result to next layewr\n",
    "        return x\n",
    "\n",
    "# Instantiate Net \n",
    "net = Net()\n",
    "model = nn.Flatten(net)\n",
    "print(model)\n",
    "print(\"\\nFlattened: \\n\", model, \"\\n As is: \\n\", net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c5966e-ef09-466b-b417-a02630b75a50",
   "metadata": {},
   "source": [
    "### Train and  Test a simple three layer nn with \"torch\" module\n",
    "\n",
    "- Import NIST data sets for training and testing\n",
    "- Define the nn stack that has three layers\n",
    "- Shows how forward method is used to define the way output is calculated\n",
    "- Show how loss function and optimiser scheme are specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58a5813-4e85-4e78-b37c-58c72e98b4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint as pp\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Vision Data sets will be used in this example\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "#Let us get two sets of data, one for training\n",
    "# and other for testing\n",
    "# Data will be downloaded form NIST if it wasn't already downloaded\n",
    "\n",
    "trainData = datasets.FashionMNIST (\n",
    "            root = \"data\",   # saves in ./data directory\n",
    "            train = True,\n",
    "            download = True,\n",
    "            transform = ToTensor ()  \n",
    "            )\n",
    "\n",
    "testData = datasets.FashionMNIST (\n",
    "            root = \"data\",   # saves in ./data directory\n",
    "            train = False,\n",
    "            download = True,\n",
    "            transform = ToTensor ()  \n",
    "            )\n",
    "\n",
    "#  Take a subset of data for our runs (with each base containing 64 data)\n",
    "size = 64\n",
    "trainLoader = DataLoader (trainData, batch_size = size)\n",
    "testLoader = DataLoader (testData, batch_size = size)\n",
    "\n",
    "# Loop through testData (y is 1D vector, X is a tensor)\n",
    "# size of y is 64, and X (64, ., .,. .)\n",
    "for X, y in testLoader:\n",
    "    print (f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print (f\"Shape of y : {y.shape}\")\n",
    "    break\n",
    "\n",
    "\n",
    "#Setup the neural Network\n",
    "\n",
    "#  device used for nn\n",
    "#print(\"cuda: \",  torch.cuda.is_available())\n",
    "#print(\"mps: \", torch.backends.mps.is_available())\n",
    "# I see that on iMac I have mps\n",
    "device = \"mps\"\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork (nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()    # Convert to one dimensinmal  vector\n",
    "        self.stack = nn.Sequential ( # Input, middle and output layers\n",
    "            nn.Linear (in_features = 28 * 28, out_features = 512), nn.ReLU (),\n",
    "            nn.Linear (512, 512), nn.ReLU (),\n",
    "            nn.Linear (512, 10)\n",
    "            )\n",
    "\n",
    "    def forward (self, x):   # is this final output calc\n",
    "         x = self.flatten (x)\n",
    "         logits = self.stack (x)\n",
    "         return logits\n",
    "\n",
    "\n",
    "# note that we dont pass any module to the nn class\n",
    "# Instantiate the model and print\n",
    "model = NeuralNetwork().to(device)  \n",
    "print(model)\n",
    "\n",
    "#With \n",
    "\n",
    "lossFn = nn.CrossEntropyLoss()   # need to know what this is\n",
    "\n",
    "#Configure optimiser (using Steepest gradient \n",
    "optimiser = torch.optim.SGD (model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training method\n",
    "def train (dataLoader, model, lossFn, optimiser):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    #Loop through all batches of data\n",
    "    xp = [] ; yp = []\n",
    "    for batch, (X, y) in enumerate (dataLoader):\n",
    "        X, y = X.to (device), y.to (device)  # transfer data to device\n",
    "\n",
    "        # predict (calculate y from network)\n",
    "        pred = model (X)\n",
    "        loss = lossFn (pred, y)\n",
    "\n",
    "        # backpropagate\n",
    "        loss.backward ()   # back propagate all derivatives?\n",
    "        optimiser.step ()\n",
    "        optimiser.zero_grad ()  # what does this do\n",
    "\n",
    "        # Print every 100 points ?\n",
    "        size = len (dataLoader.dataset)\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len (X)\n",
    "            print (f\"loss: {loss:>7f} [{current:>5d} / {size:>5d}]\")\n",
    "            xp.append (current)\n",
    "            yp.append (loss)\n",
    "\n",
    "    return ({\"xp\": xp, \"yp\": yp})\n",
    "#   End of Train Method\n",
    "\n",
    "# Testing method \n",
    "def test (dataLoader, model, lossFn):\n",
    "    size = len (dataLoader.dataset) \n",
    "    nb = len (dataLoader) \n",
    "    model.eval ()\n",
    "    loss, correct = 0, 0\n",
    "    with torch.no_grad() :\n",
    "        for X, y in dataLoader:\n",
    "            X, y = X.to (device), y.to (device)\n",
    "            pred = model (X)\n",
    "            loss +=  lossFn (pred, y).item()\n",
    "            # What a cryptic coding\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item() \n",
    "        loss /= nb\n",
    "        correct /= size\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {loss:>8f} \\n\")\n",
    "\n",
    "plt.ion() \n",
    "\n",
    "fig = plt.figure() ;\n",
    "plt.xlabel(\"data count\") ; plt.ylabel(\"loss\")\n",
    "plt.grid() \n",
    "for t in range(5):\n",
    "    data = train(trainLoader, model, lossFn, optimiser)\n",
    "    test(testLoader, model, lossFn)\n",
    "    plt.plot (data[\"xp\"], data[\"yp\"], '-o')  ; plt.grid()\n",
    "    plt.draw ()\n",
    "    #plt.pause(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504410de-e2b3-40af-b66a-1f83c5822360",
   "metadata": {},
   "source": [
    "### Display Test Data graphically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baab0955-4da2-4abb-aa46-73d721406f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to see the image data we pulled in\n",
    "def testme():\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    # Vision Data sets will be used in this example\n",
    "    from torchvision import datasets\n",
    "    from torchvision.transforms import ToTensor\n",
    "\n",
    "    #Let us get two sets of data, one for training\n",
    "    # and other for testing\n",
    "    # Data will be downloaded form NIST if it wasn't already downloaded\n",
    "\n",
    "    trainData = datasets.FashionMNIST (\n",
    "                root = \"data\",   # saves in ./data directory\n",
    "                train = True,\n",
    "                download = True,\n",
    "                transform = ToTensor ()  \n",
    "                )\n",
    "    dataset = DataLoader (trainData, batch_size = 100).dataset\n",
    "    n = len(dataset)\n",
    "    fig = plt.figure(figsize=(7,7)) ## what this size represent? \n",
    "\n",
    "    labels_map = {\n",
    "        0: \"T-Shirt\",\n",
    "        1: \"Trouser\",\n",
    "        2: \"Pullover\",\n",
    "        3: \"Dress\",\n",
    "        4: \"Coat\",\n",
    "        5: \"Sandal\",\n",
    "        6: \"Shirt\",\n",
    "        7: \"Sneaker\",\n",
    "        8: \"Bag\",\n",
    "        9: \"Ankle Boot\"\n",
    "    }\n",
    "    for i in range(9):\n",
    "        idx = torch.randint( len(dataset), size=(1,)).item()\n",
    "        img, label = dataset[idx]\n",
    "        fig.add_subplot(3, 3, i+1)\n",
    "        plt.title(labels_map[label])\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    print(i)\n",
    "###########################\n",
    "testme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefa7c0d-6cc8-4dc9-acd4-97ae1832a8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
